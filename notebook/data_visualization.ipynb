{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataScience-Instruct-500K — Data Exploration\n",
    "\n",
    "This notebook provides an overview and samples data from every file in the [RUC-DataLab/DataScience-Instruct-500K](https://huggingface.co/datasets/RUC-DataLab/DataScience-Instruct-500K) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "DATA_ROOT = \"../data/DataScience-Instruct-500K\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dataset Overview — File Counts & Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 37\n",
      "Total size: 13452.2 MB\n",
      "\n",
      "            num_files  total_mb\n",
      "folder                         \n",
      "RL                  5    3281.9\n",
      "assets              2       0.7\n",
      "interation         12    1240.5\n",
      "reasoning          15    3913.1\n",
      "root                3    5016.0\n"
     ]
    }
   ],
   "source": [
    "# Collect file metadata\n",
    "file_info = []\n",
    "for root, dirs, files in os.walk(DATA_ROOT):\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    for f in files:\n",
    "        if f.startswith('.'):\n",
    "            continue\n",
    "        fpath = os.path.join(root, f)\n",
    "        rel = os.path.relpath(fpath, DATA_ROOT)\n",
    "        folder = rel.split('/')[0] if '/' in rel else 'root'\n",
    "        size_mb = os.path.getsize(fpath) / (1024**2)\n",
    "        file_info.append({'file': rel, 'folder': folder, 'size_mb': size_mb, 'ext': os.path.splitext(f)[1]})\n",
    "\n",
    "df_files = pd.DataFrame(file_info)\n",
    "print(f\"Total files: {len(df_files)}\")\n",
    "print(f\"Total size: {df_files['size_mb'].sum():.1f} MB\")\n",
    "print()\n",
    "print(df_files.groupby('folder').agg(num_files=('file', 'count'), total_mb=('size_mb', 'sum')).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sample Counts per Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples across subsets: 516,055\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>SKGInstruct_199989.json</td>\n",
       "      <td>SKGInstruct</td>\n",
       "      <td>199989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>TableQA_distillation_39301.json</td>\n",
       "      <td>TableQA_distillation</td>\n",
       "      <td>39301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>TableQA_refinement_39301.json</td>\n",
       "      <td>TableQA_refinement</td>\n",
       "      <td>39301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>TableQA_original_35357.json</td>\n",
       "      <td>TableQA_original</td>\n",
       "      <td>35357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>TableGPT_29448.json</td>\n",
       "      <td>TableGPT</td>\n",
       "      <td>29448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>instruction_following_20000.json</td>\n",
       "      <td>instruction_following</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>code_20000.json</td>\n",
       "      <td>code</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>science_20000.json</td>\n",
       "      <td>science</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>math_20000.json</td>\n",
       "      <td>math</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>other_19998.json</td>\n",
       "      <td>other</td>\n",
       "      <td>19998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>dscode_16338.json</td>\n",
       "      <td>dscode</td>\n",
       "      <td>16338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RL</td>\n",
       "      <td>reseach.parquet</td>\n",
       "      <td>reseach</td>\n",
       "      <td>9246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_report_generation_4327.json</td>\n",
       "      <td>research_report_generation</td>\n",
       "      <td>4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>interation</td>\n",
       "      <td>data_analysis_3936.json</td>\n",
       "      <td>data_analysis</td>\n",
       "      <td>3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RL</td>\n",
       "      <td>datatask.parquet</td>\n",
       "      <td>datatask</td>\n",
       "      <td>3852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>file_database_3833.json</td>\n",
       "      <td>file_database</td>\n",
       "      <td>3833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>file_xlsx_3663.json</td>\n",
       "      <td>file_xlsx</td>\n",
       "      <td>3663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>interation</td>\n",
       "      <td>data_pipeline_3601.json</td>\n",
       "      <td>data_pipeline</td>\n",
       "      <td>3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_other_3505.json</td>\n",
       "      <td>research_other</td>\n",
       "      <td>3505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>interation</td>\n",
       "      <td>data_preparation_3311.json</td>\n",
       "      <td>data_preparation</td>\n",
       "      <td>3311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>file_csv_3007.json</td>\n",
       "      <td>file_csv</td>\n",
       "      <td>3007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>file_any_2520.json</td>\n",
       "      <td>file_any</td>\n",
       "      <td>2520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RL</td>\n",
       "      <td>qa.parquet</td>\n",
       "      <td>qa</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RL</td>\n",
       "      <td>reseach_small.parquet</td>\n",
       "      <td>reseach_small</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>interation</td>\n",
       "      <td>data_cleaning_1616.json</td>\n",
       "      <td>data_cleaning</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_data_insight_1351.json</td>\n",
       "      <td>research_data_insight</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_data_analysis_1339.json</td>\n",
       "      <td>research_data_analysis</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>interation</td>\n",
       "      <td>data_insight_1062.json</td>\n",
       "      <td>data_insight</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_xlsx_848.json</td>\n",
       "      <td>research_xlsx</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_database_818.json</td>\n",
       "      <td>research_database</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>interation</td>\n",
       "      <td>research_data_preparation_488.json</td>\n",
       "      <td>research_data_preparation</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        folder                                  file  \\\n",
       "0    reasoning               SKGInstruct_199989.json   \n",
       "1    reasoning       TableQA_distillation_39301.json   \n",
       "2    reasoning         TableQA_refinement_39301.json   \n",
       "3    reasoning           TableQA_original_35357.json   \n",
       "4    reasoning                   TableGPT_29448.json   \n",
       "5    reasoning      instruction_following_20000.json   \n",
       "6    reasoning                       code_20000.json   \n",
       "7    reasoning                    science_20000.json   \n",
       "8    reasoning                       math_20000.json   \n",
       "9    reasoning                      other_19998.json   \n",
       "10   reasoning                     dscode_16338.json   \n",
       "11          RL                       reseach.parquet   \n",
       "12  interation  research_report_generation_4327.json   \n",
       "13  interation               data_analysis_3936.json   \n",
       "14          RL                      datatask.parquet   \n",
       "15   reasoning               file_database_3833.json   \n",
       "16   reasoning                   file_xlsx_3663.json   \n",
       "17  interation               data_pipeline_3601.json   \n",
       "18  interation              research_other_3505.json   \n",
       "19  interation            data_preparation_3311.json   \n",
       "20   reasoning                    file_csv_3007.json   \n",
       "21   reasoning                    file_any_2520.json   \n",
       "22          RL                            qa.parquet   \n",
       "23          RL                 reseach_small.parquet   \n",
       "24  interation               data_cleaning_1616.json   \n",
       "25  interation       research_data_insight_1351.json   \n",
       "26  interation      research_data_analysis_1339.json   \n",
       "27  interation                data_insight_1062.json   \n",
       "28  interation                research_xlsx_848.json   \n",
       "29  interation            research_database_818.json   \n",
       "30  interation    research_data_preparation_488.json   \n",
       "\n",
       "                      category   count  \n",
       "0                  SKGInstruct  199989  \n",
       "1         TableQA_distillation   39301  \n",
       "2           TableQA_refinement   39301  \n",
       "3             TableQA_original   35357  \n",
       "4                     TableGPT   29448  \n",
       "5        instruction_following   20000  \n",
       "6                         code   20000  \n",
       "7                      science   20000  \n",
       "8                         math   20000  \n",
       "9                        other   19998  \n",
       "10                      dscode   16338  \n",
       "11                     reseach    9246  \n",
       "12  research_report_generation    4327  \n",
       "13               data_analysis    3936  \n",
       "14                    datatask    3852  \n",
       "15               file_database    3833  \n",
       "16                   file_xlsx    3663  \n",
       "17               data_pipeline    3601  \n",
       "18              research_other    3505  \n",
       "19            data_preparation    3311  \n",
       "20                    file_csv    3007  \n",
       "21                    file_any    2520  \n",
       "22                          qa    2000  \n",
       "23               reseach_small    2000  \n",
       "24               data_cleaning    1616  \n",
       "25       research_data_insight    1351  \n",
       "26      research_data_analysis    1339  \n",
       "27                data_insight    1062  \n",
       "28               research_xlsx     848  \n",
       "29           research_database     818  \n",
       "30   research_data_preparation     488  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count samples in each file\n",
    "sample_counts = []\n",
    "\n",
    "for folder in ['interation', 'reasoning']:\n",
    "    json_files = sorted(glob.glob(os.path.join(DATA_ROOT, folder, '*.json')))\n",
    "    for fpath in json_files:\n",
    "        fname = os.path.basename(fpath)\n",
    "        parts = fname.replace('.json', '').rsplit('_', 1)\n",
    "        count = int(parts[-1]) if parts[-1].isdigit() else None\n",
    "        category = parts[0] if count is not None else fname.replace('.json', '')\n",
    "        sample_counts.append({'folder': folder, 'file': fname, 'category': category, 'count': count})\n",
    "\n",
    "for fpath in sorted(glob.glob(os.path.join(DATA_ROOT, 'RL', '*.parquet'))):\n",
    "    fname = os.path.basename(fpath)\n",
    "    df_tmp = pd.read_parquet(fpath)\n",
    "    sample_counts.append({'folder': 'RL', 'file': fname, 'category': fname.replace('.parquet', ''), 'count': len(df_tmp)})\n",
    "\n",
    "df_counts = pd.DataFrame(sample_counts)\n",
    "print(f\"Total samples across subsets: {df_counts['count'].sum():,}\")\n",
    "print()\n",
    "display(df_counts.sort_values('count', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Sample Data — Root-level JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE: single_ability_finetuning.json\n",
      "================================================================================\n",
      "Total samples: 437,398\n",
      "Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: You are capable of effectively identifying the hierarchical structure of the table. Based on the provided table and textual description, please provide the answer to the question.\n",
      "You should think ste\n",
      "    [assistant]: <Analyze>\n",
      "First, the question is: \"comparing to 2015, how many percentage point has nova scotia decreased in their crime severity index (csi) in 2016?\"\n",
      "\n",
      "I need to find the percentage change for Nova S\n",
      "  input_tokens: 722\n",
      "  output_tokens: 3562\n",
      "  total_tokens: 4284\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 4, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "--- Sample 1 ---\n",
      "  id: 1\n",
      "  messages (2 msgs):\n",
      "    [user]: Please answer the given question based on the table and text. You should reach a short-form answer after reasoning.\n",
      "### Instruction\n",
      "Given a table and a list of texts in the following, what is the answ\n",
      "    [assistant]: <Analyze>\n",
      "Alright, let's tackle this question step by step. The question is: \"Which group did the person who directed The Lonely Killers establish?\" \n",
      "\n",
      "First, I need to identify who directed \"The Lonel\n",
      "  input_tokens: 1259\n",
      "  output_tokens: 696\n",
      "  total_tokens: 1955\n",
      "  evaluation: {\"difficulty\": 2, \"quality\": 5, \"ability\": \"Reasoning\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: multi_ability_agentic_training.json\n",
      "================================================================================\n",
      "Total samples: 26,202\n",
      "Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      " Generate a data analysis report linking player performance metrics (e.g., GP, PIM, PLUSMINUS) and physical attributes (e.g., height, weight) to key performance indicators (KPIs) such a\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data analysis, I will first establish a structured approach to examine the relationship between player physical attributes and performance metrics. The analysis w\n",
      "  input_tokens: 357\n",
      "  output_tokens: 15821\n",
      "  total_tokens: 16178\n",
      "\n",
      "--- Sample 1 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a data analysis report that follows the strategic recommendations report outline including Executive Summary, Introduction, Data Sources and Quality, Analytical Approach, Findi\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data analysis task, I will first need to understand the structure and content of each provided dataset. The files include height_info.xlsx, weight_info.xlsx, Play\n",
      "  input_tokens: 175\n",
      "  output_tokens: 9531\n",
      "  total_tokens: 9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname in ['single_ability_finetuning.json', 'multi_ability_agentic_training.json']:\n",
    "    fpath = os.path.join(DATA_ROOT, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        continue\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"FILE: {fname}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    with open(fpath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Total samples: {len(data):,}\")\n",
    "    print(f\"Keys: {list(data[0].keys())}\")\n",
    "    print()\n",
    "    for i, entry in enumerate(data[:1]):\n",
    "        print(f\"--- Sample {i} ---\")\n",
    "        for k, v in entry.items():\n",
    "            if k == 'messages':\n",
    "                print(f\"  messages ({len(v)} msgs):\")\n",
    "                for msg in v[:3]:\n",
    "                    content_preview = str(msg['content'])[:200]\n",
    "                    print(f\"    [{msg['role']}]: {content_preview}\")\n",
    "                if len(v) > 3:\n",
    "                    print(f\"    ... ({len(v)-3} more messages)\")\n",
    "            elif k == 'evaluation':\n",
    "                print(f\"  evaluation: {json.dumps(v, ensure_ascii=False)[:300]}\")\n",
    "            else:\n",
    "                print(f\"  {k}: {str(v)[:200]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Sample Data — Interaction Files (`interation/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE: interation/data_analysis_3936.json\n",
      "================================================================================\n",
      "Total samples: 3,936  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 4\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "Integrate the 'all_star' and 'fielding' tables via player_id and year to categorize players as all-stars (present in 'all_star') or non-all-stars (present in 'fielding' but not 'all_star\n",
      "    [assistant]: <Analyze>\n",
      "I'll begin by carefully planning the analysis approach for this task. The instruction involves multiple complex steps that need to be executed sequentially. Here's my initial plan:\n",
      "\n",
      "1. Data \n",
      "  evaluation: {\"difficulty\": 5, \"quality\": 5, \"ability\": [\"Data Preparation\", \"Data Analysis\", \"Data Visualization\", \"Data Modeling\", \"Data Insight\", \"Code Generation\", \"Reasoning\", \"Numerical Calculation\", \"Instruction Following\", \"Structural Understanding\"]}\n",
      "  input_tokens: 299\n",
      "  output_tokens: 10456\n",
      "  total_tokens: 10755\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/data_cleaning_1616.json\n",
      "================================================================================\n",
      "Total samples: 1,616  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "Perform a comprehensive data cleaning and validation workflow on the provided `driver` and `school_bus` tables, addressing the following tasks:\n",
      "\n",
      "1. **Schema Validation & Cross-Table Inte\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data cleaning and validation workflow, I'll first need to understand the structure and content of both datasets. Since we have an Excel file for the driver data a\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": [\"Data Preparation\", \"Data Analysis\", \"Data Modeling\", \"Data Insight\", \"Code Generation\", \"Reasoning\", \"Numerical Calculation\", \"Instruction Following\", \"Structural Understanding\"]}\n",
      "  input_tokens: 605\n",
      "  output_tokens: 5056\n",
      "  total_tokens: 5661\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/data_insight_1062.json\n",
      "================================================================================\n",
      "Total samples: 1,062  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 3000\n",
      "  messages (2 msgs):\n",
      "    [user]: How do the starting positions of All-Star players (from all_star.xlsx) relate to their outfield fielding games (glf, gcf, grf in fielding_outfield.xlsx) for the same year?\n",
      "    [assistant]: <Analyze>\n",
      "To address this task, we need to understand the relationship between All-Star players' starting positions and their outfield fielding statistics for the same year. Here's the step-by-step pl\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": [\"Data Preparation\", \"Data Analysis\", \"Data Visualization\", \"Data Modeling\", \"Data Insight\", \"Code Generation\", \"Reasoning\", \"Instruction Following\", \"Structural Understanding\"]}\n",
      "  input_tokens: 43\n",
      "  output_tokens: 6577\n",
      "  total_tokens: 6620\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/data_pipeline_3601.json\n",
      "================================================================================\n",
      "Total samples: 3,601  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 7000\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "The datasets `all_star.csv` and `fielding_postseason.csv` contain information about Major League Baseball players' All-Star game participation and postseason fielding performance, respec\n",
      "    [assistant]: <Analyze>\n",
      "Let me break down the task and plan my approach:\n",
      "\n",
      "1. Data Understanding Phase:\n",
      "- First need to examine both datasets to understand their structure, columns, and data quality\n",
      "- Will check for\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": [\"Data Preparation\", \"Data Analysis\", \"Data Modeling\", \"Data Insight\", \"Code Generation\", \"Reasoning\", \"Instruction Following\", \"Structural Understanding\"]}\n",
      "  input_tokens: 535\n",
      "  output_tokens: 10348\n",
      "  total_tokens: 10883\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/data_preparation_3311.json\n",
      "================================================================================\n",
      "Total samples: 3,311  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 3000\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "Perform the following data transformation tasks using the 'all_star' and 'team' tables: \n",
      "\n",
      "1. **Merge** the two tables using the common keys (team_id, year, league_id) to create a combine\n",
      "    [assistant]: <Analyze>\n",
      "Let me break down the task systematically:\n",
      "\n",
      "1. **Data Understanding Phase**:\n",
      "- First need to inspect both CSV files to understand their structure and columns\n",
      "- Verify the common keys (team_i\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": [\"Data Preparation\", \"Data Analysis\", \"Data Visualization\", \"Data Modeling\", \"Data Insight\", \"Code Generation\", \"Reasoning\", \"Numerical Calculation\", \"Instruction Following\", \"Structural Understanding\"]}\n",
      "  input_tokens: 513\n",
      "  output_tokens: 5070\n",
      "  total_tokens: 5583\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_data_analysis_1339.json\n",
      "================================================================================\n",
      "Total samples: 1,339  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 1\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Perform an Exploratory Data Analysis (EDA) on the provided football match and division datasets to uncover patterns, trends, and insights, and generate a comprehensive report.\n",
      "## Data\n",
      "F\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive exploratory data analysis of football match data, I'll first establish a systematic approach to understanding and analyzing the datasets. The two files provided -\n",
      "  input_tokens: 92\n",
      "  output_tokens: 21117\n",
      "  total_tokens: 21209\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_data_insight_1351.json\n",
      "================================================================================\n",
      "Total samples: 1,351  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 1\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a data insight report from the provided football matches data, highlighting key observations and trends in match results, goals scored, and team performance across different se\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive football match data analysis, I'll first need to understand the structure and content of the provided matchs.csv file. The analysis will proceed through several k\n",
      "  input_tokens: 67\n",
      "  output_tokens: 10118\n",
      "  total_tokens: 10185\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_data_preparation_488.json\n",
      "================================================================================\n",
      "Total samples: 488  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 1\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a report that combines the 'divisions' and 'matchs' tables to analyze the number of home wins, away wins, and draws for each league division, detailing the data preprocessing s\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive analysis of football match outcomes across different league divisions, I will develop a detailed multi-step approach that ensures thorough data exploration, clean\n",
      "  input_tokens: 112\n",
      "  output_tokens: 20746\n",
      "  total_tokens: 20858\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_database_818.json\n",
      "================================================================================\n",
      "Total samples: 818  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a data report for the ice_hockey_draft.sqlite database\n",
      "## Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"ice_hockey_draft.sqlite\",\n",
      "    \"size\": \"1004.0KB\"\n",
      "}\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive analysis of the ice_hockey_draft.sqlite database, I will first need to understand the structure and contents of the database. Since this is a SQLite database, it \n",
      "  input_tokens: 49\n",
      "  output_tokens: 21624\n",
      "  total_tokens: 21673\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_other_3505.json\n",
      "================================================================================\n",
      "Total samples: 3,505  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 4500\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a comprehensive data science report analyzing the performance trends of All-Star players across different decades, focusing on the following aspects:  \n",
      "1. Compare the average n\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data science report, I will first need to understand the structure and contents of both provided datasets (all_star.csv and team.csv). The analysis will proceed t\n",
      "  input_tokens: 292\n",
      "  output_tokens: 19751\n",
      "  total_tokens: 20043\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_report_generation_4327.json\n",
      "================================================================================\n",
      "Total samples: 4,327  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      " Generate a data analysis report linking player performance metrics (e.g., GP, PIM, PLUSMINUS) and physical attributes (e.g., height, weight) to key performance indicators (KPIs) such a\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data analysis, I will first establish a structured approach to examine the relationship between player physical attributes and performance metrics. The analysis w\n",
      "  input_tokens: 357\n",
      "  output_tokens: 15821\n",
      "  total_tokens: 16178\n",
      "\n",
      "================================================================================\n",
      "FILE: interation/research_xlsx_848.json\n",
      "================================================================================\n",
      "Total samples: 848  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Instruction\n",
      "Generate a comprehensive data science report analyzing the relationship between player physical attributes (height and weight) and their performance metrics (goals, assists, points, and\n",
      "    [assistant]: <Analyze>\n",
      "To begin this comprehensive data science report, I will first need to understand the structure and content of the provided datasets. The analysis will proceed through several key phases:\n",
      "\n",
      "1.\n",
      "  input_tokens: 331\n",
      "  output_tokens: 9213\n",
      "  total_tokens: 9544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interation_files = sorted(glob.glob(os.path.join(DATA_ROOT, 'interation', '*.json')))\n",
    "\n",
    "for fpath in interation_files:\n",
    "    fname = os.path.basename(fpath)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"FILE: interation/{fname}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    with open(fpath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Total samples: {len(data):,}  |  Keys: {list(data[0].keys())}\")\n",
    "    \n",
    "    entry = data[0]\n",
    "    print(f\"\\n--- Sample 0 ---\")\n",
    "    for k, v in entry.items():\n",
    "        if k == 'messages':\n",
    "            print(f\"  messages ({len(v)} msgs):\")\n",
    "            for msg in v[:3]:\n",
    "                content_preview = str(msg['content'])[:200]\n",
    "                print(f\"    [{msg['role']}]: {content_preview}\")\n",
    "            if len(v) > 3:\n",
    "                print(f\"    ... ({len(v)-3} more messages)\")\n",
    "        elif k == 'evaluation':\n",
    "            print(f\"  evaluation: {json.dumps(v, ensure_ascii=False)[:300]}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {str(v)[:200]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sample Data — Reasoning Files (`reasoning/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE: reasoning/SKGInstruct_199989.json\n",
      "================================================================================\n",
      "Total samples: 199,989  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 512069\n",
      "  messages (2 msgs):\n",
      "    [user]: Write your answer to the question based on your reasoning given the information in the following table table:\n",
      "\n",
      "col : stem | leaf  row 1 : 3 | 3, 3, 3, 5, 5 row 2 : 4 | 6 row 3 : 5 | 4, 5, 7, 8 row 4 :\n",
      "    [assistant]: <Analyze>\n",
      "To solve the problem, we need to determine how many gardens have at least 47 plants based on the given stem-and-leaf plot. Here's the step-by-step plan:\n",
      "\n",
      "1. **Interpret the Stem-and-Leaf Plo\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 5, \"ability\": \"Reasoning\"}\n",
      "  input_tokens: 151\n",
      "  output_tokens: 789\n",
      "  total_tokens: 940\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/TableGPT_29448.json\n",
      "================================================================================\n",
      "Total samples: 29,448  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "Given the input table data and the list of headers that are potential candidates, your role is to choose the most suitable header for each column in the table. Choose exclusively from th\n",
      "    [assistant]: <Analyze>\n",
      "To solve this task, I need to match each column in the provided table data with the most suitable header from the candidate list. The approach involves:\n",
      "1. Understanding the nature of the da\n",
      "  input_tokens: 361\n",
      "  output_tokens: 2121\n",
      "  total_tokens: 2482\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": \"Reasoning\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/TableQA_distillation_39301.json\n",
      "================================================================================\n",
      "Total samples: 39,301  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: You are capable of effectively identifying the hierarchical structure of the table. Based on the provided table and textual description, please provide the answer to the question.\n",
      "You should think ste\n",
      "    [assistant]: <Analyze>\n",
      "First, the question is: \"comparing to 2015, how many percentage point has nova scotia decreased in their crime severity index (csi) in 2016?\"\n",
      "\n",
      "I need to find the percentage change for Nova S\n",
      "  input_tokens: 722\n",
      "  output_tokens: 3562\n",
      "  total_tokens: 4284\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 4, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/TableQA_original_35357.json\n",
      "================================================================================\n",
      "Total samples: 35,357  |  Keys: ['id', 'messages']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Objective\n",
      "You are provided with a table and a question related to the table.\n",
      "Please generate a SQL query to answer this question. You should think step-by-step to reach the final SQL query.\n",
      "\n",
      "## Tab\n",
      "    [assistant]: <Analyze>\n",
      "Okay, I need to figure out the SQL query to find the format for South Australia. Let's start by looking at the table provided. The table has several columns: State/territory, Text/background\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/TableQA_refinement_39301.json\n",
      "================================================================================\n",
      "Total samples: 39,301  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: You are capable of effectively identifying the hierarchical structure of the table. Based on the provided table and textual description, please provide the answer to the question.\n",
      "You should think ste\n",
      "    [assistant]: <Analyze>\n",
      "Let's begin by understanding the question: \"comparing to 2015, how many percentage point has nova scotia decreased in their crime severity index (csi) in 2016?\" To answer this, I need to ide\n",
      "  input_tokens: 722\n",
      "  output_tokens: 2533\n",
      "  total_tokens: 3255\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 4, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/code_20000.json\n",
      "================================================================================\n",
      "Total samples: 20,000  |  Keys: ['id', 'messages', 'evaluation', 'input_tokens', 'output_tokens', 'total_tokens']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: ## Imputation of Missing Values\n",
      "\n",
      "**Objective**: Implement and compare different imputation strategies to handle missing values in a dataset.\n",
      "\n",
      "### Problem Statement:\n",
      "Consider the following dataset repr\n",
      "    [assistant]: <Analyze>\n",
      "We are going to create a pandas DataFrame for the given dataset.\n",
      " Then, we will define three functions for imputation: simple_imputer, knn_imputer, and iterative_imputer.\n",
      " Each function will\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": \"Data Preparation\"}\n",
      "  input_tokens: 992\n",
      "  output_tokens: 1385\n",
      "  total_tokens: 2377\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/dscode_16338.json\n",
      "================================================================================\n",
      "Total samples: 16,338  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: Create a regression model to predict the value of a particular attribute in a dataset.\n",
      "    [assistant]: <Analyze>\n",
      "We are going to create a regression model. The steps typically involve:\n",
      " 1. Loading the dataset.\n",
      " 2. Exploring and preprocessing the data (handling missing values, encoding categorical varia\n",
      "  input_tokens: 16\n",
      "  output_tokens: 2861\n",
      "  total_tokens: 2877\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": \"Data Modeling\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/file_any_2520.json\n",
      "================================================================================\n",
      "Total samples: 2,520  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "How many heads of the departments are older than 56 ?\n",
      "# Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"head.json\",\n",
      "    \"size\": \"0.7KB\"\n",
      "}\n",
      "    [assistant]: <Analyze>\n",
      "To solve this problem, we need to determine how many department heads are older than 56. The data is provided in a JSON file named \"head.json\". Here's the step-by-step plan:\n",
      "\n",
      "1. First, we ne\n",
      "  input_tokens: 43\n",
      "  output_tokens: 2566\n",
      "  total_tokens: 2609\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 5, \"ability\": \"Data Preparation\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/file_csv_3007.json\n",
      "================================================================================\n",
      "Total samples: 3,007  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "How many heads of the departments are older than 56 ?\n",
      "# Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"head.csv\",\n",
      "    \"size\": \"0.3KB\"\n",
      "}\n",
      "    [assistant]: <Analyze>\n",
      "To solve this problem, we need to determine how many department heads are older than 56. The only data provided is a file named \"head.csv\". Since we don't know the structure of this file, we\n",
      "  input_tokens: 43\n",
      "  output_tokens: 1854\n",
      "  total_tokens: 1897\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 5, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/file_database_3833.json\n",
      "================================================================================\n",
      "Total samples: 3,833  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "How many heads of the departments are older than 56 ?\n",
      "# Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"0.db\",\n",
      "    \"size\": \"8.0KB\"\n",
      "}\n",
      "    [assistant]: <Analyze>\n",
      "To solve this problem, we need to determine how many department heads are older than 56. However, we currently only have a filename (\"0.db\") without knowing its structure or content. Here's \n",
      "  input_tokens: 43\n",
      "  output_tokens: 1642\n",
      "  total_tokens: 1685\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 4, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/file_xlsx_3663.json\n",
      "================================================================================\n",
      "Total samples: 3,663  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: # Instruction\n",
      "How many heads of the departments are older than 56 ?\n",
      "# Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"head.xlsx\",\n",
      "    \"size\": \"5.2KB\"\n",
      "}\n",
      "    [assistant]: <Analyze>\n",
      "To answer the question \"How many heads of the departments are older than 56?\", I need to:\n",
      "\n",
      "1. Understand the data structure in head.xlsx:\n",
      "   - It likely contains information about department\n",
      "  input_tokens: 43\n",
      "  output_tokens: 1895\n",
      "  total_tokens: 1938\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 5, \"ability\": \"Data Analysis\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/instruction_following_20000.json\n",
      "================================================================================\n",
      "Total samples: 20,000  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: Your response should contain at least 5 sentences. The response must contain at least 2 placeholders represented by square brackets, such as [address]. Include keywords [dog], [walk], and [park] in th\n",
      "    [assistant]: <Analyze>\n",
      "Hmm, the user wants me to create a response with specific requirements. They need at least five sentences, with two placeholders in square brackets like [address]. I also need to include the\n",
      "  input_tokens: 62\n",
      "  output_tokens: 459\n",
      "  total_tokens: 521\n",
      "  evaluation: {\"difficulty\": 2, \"quality\": 4, \"ability\": \"Instruction Following\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/math_20000.json\n",
      "================================================================================\n",
      "Total samples: 20,000  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: Find the $1314^{\\text{th}}$ digit past the decimal point in the decimal expansion of $\\dfrac{5}{14}$.\n",
      "    [assistant]: <Analyze>\n",
      "I need to find the 1314th digit after the decimal point in the decimal expansion of 5/14. First, I know that fractions can have repeating decimals, especially when the denominator has prime \n",
      "  input_tokens: 34\n",
      "  output_tokens: 13142\n",
      "  total_tokens: 13176\n",
      "  evaluation: {\"difficulty\": 4, \"quality\": 5, \"ability\": \"Numerical Calculation\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/other_19998.json\n",
      "================================================================================\n",
      "Total samples: 19,998  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: Could you rephrase the sentence \"Caitlin’s outdone herself with this one! I love it!\" in simpler terms, and also reflect it to imply occasional readiness or lack thereof in facing adult responsibiliti\n",
      "    [assistant]: <Analyze>\n",
      "Okay, the user wants me to rephrase a positive statement about Caitlin's accomplishment while also weaving in a reflection about adult responsibilities. \n",
      "\n",
      "Hmm, they've given me two clear par\n",
      "  input_tokens: 82\n",
      "  output_tokens: 963\n",
      "  total_tokens: 1045\n",
      "  evaluation: {\"difficulty\": 3, \"quality\": 4, \"ability\": \"Reasoning\"}\n",
      "\n",
      "================================================================================\n",
      "FILE: reasoning/science_20000.json\n",
      "================================================================================\n",
      "Total samples: 20,000  |  Keys: ['id', 'messages', 'input_tokens', 'output_tokens', 'total_tokens', 'evaluation']\n",
      "\n",
      "--- Sample 0 ---\n",
      "  id: 0\n",
      "  messages (2 msgs):\n",
      "    [user]: In an RLC series circuit, the quality factor \\( Q \\) is defined as the ratio of the resonant frequency \\( \\omega_0 \\) to the bandwidth \\( \\Delta \\omega \\) at half-power points. If the resonant frequen\n",
      "    [assistant]: <Analyze>\n",
      "The problem is about an RLC series circuit and the quality factor Q. It's defined as the ratio of the resonant frequency ω₀ to the bandwidth Δω at the half-power points. Given that ω₀ is 100\n",
      "  input_tokens: 156\n",
      "  output_tokens: 1736\n",
      "  total_tokens: 1892\n",
      "  evaluation: {\"difficulty\": 2, \"quality\": 5, \"ability\": \"Numerical Calculation\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reasoning_files = sorted(glob.glob(os.path.join(DATA_ROOT, 'reasoning', '*.json')))\n",
    "\n",
    "for fpath in reasoning_files:\n",
    "    fname = os.path.basename(fpath)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"FILE: reasoning/{fname}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    with open(fpath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Total samples: {len(data):,}  |  Keys: {list(data[0].keys())}\")\n",
    "    \n",
    "    entry = data[0]\n",
    "    print(f\"\\n--- Sample 0 ---\")\n",
    "    for k, v in entry.items():\n",
    "        if k == 'messages':\n",
    "            print(f\"  messages ({len(v)} msgs):\")\n",
    "            for msg in v[:3]:\n",
    "                content_preview = str(msg['content'])[:200]\n",
    "                print(f\"    [{msg['role']}]: {content_preview}\")\n",
    "            if len(v) > 3:\n",
    "                print(f\"    ... ({len(v)-3} more messages)\")\n",
    "        elif k == 'evaluation':\n",
    "            print(f\"  evaluation: {json.dumps(v, ensure_ascii=False)[:300]}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {str(v)[:200]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sample Data — RL Files (`RL/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE: RL/datatask.parquet\n",
      "================================================================================\n",
      "Shape: (3852, 9)  |  Columns: ['input_seq', 'output_seq', 'workspace_id', 'external_knowledge', 'data', 'data_source', 'prompt', 'env_class', 'reward_spec']\n",
      "Dtypes:\n",
      "input_seq                str\n",
      "output_seq               str\n",
      "workspace_id             str\n",
      "external_knowledge       str\n",
      "data                     str\n",
      "data_source              str\n",
      "prompt                object\n",
      "env_class                str\n",
      "reward_spec           object\n",
      "\n",
      "--- First row ---\n",
      "  input_seq: # Instruction\n",
      "What is the count of customers that Steve Johnson supports?\n",
      "# Data\n",
      "File 1:\n",
      "{\n",
      "    \"name\": \"employees.json\",\n",
      "    \"size\": \"2.8KB\"\n",
      "}\n",
      "File 2:\n",
      "{\n",
      "    \"name\": \"customers.csv\",\n",
      "    \"size\": \"6.7KB\"\n",
      "}\n",
      "  output_seq: <Analyze>\n",
      "To determine the count of customers supported by Steve Johnson, we need to:\n",
      "1. Understand the relationship between employees and customers\n",
      "2. Identify how \"support\" is represented in the data (likely through a foreign key relationship)\n",
      "3. Locate Steve Johnson in the employees data\n",
      "4. Find \n",
      "  workspace_id: 583\n",
      "  external_knowledge: \n",
      "  data: datatask\n",
      "  data_source: datatask\n",
      "  prompt: [{'content': '# Instruction\\nWhat is the count of customers that Steve Johnson supports?\\n# Data\\nFile 1:\\n{\\n    \"name\": \"employees.json\",\\n    \"size\": \"2.8KB\"\\n}\\nFile 2:\\n{\\n    \"name\": \"customers.csv\",\\n    \"size\": \"6.7KB\"\\n}', 'role': 'user'}]\n",
      "  env_class: deepanalyze\n",
      "  reward_spec: {'function': array(['llm_as_judgement_accuracy', 'llm_as_judgement_analyze'],\n",
      "      dtype=object), 'ground_truth': 'The count of customers that Steve Johnson supports is 18.', 'method': 'datatask', 'response': '<Analyze>\\nTo determine the count of customers supported by Steve Johnson, we need to:\\n1\n",
      "\n",
      "================================================================================\n",
      "FILE: RL/qa.parquet\n",
      "================================================================================\n",
      "Shape: (2000, 9)  |  Columns: ['input_seq', 'output_seq', 'workspace_id', 'external_knowledge', 'data', 'data_source', 'prompt', 'env_class', 'reward_spec']\n",
      "Dtypes:\n",
      "input_seq                str\n",
      "output_seq               str\n",
      "workspace_id             str\n",
      "external_knowledge       str\n",
      "data                     str\n",
      "data_source              str\n",
      "prompt                object\n",
      "env_class                str\n",
      "reward_spec           object\n",
      "\n",
      "--- First row ---\n",
      "  input_seq: ## Objective\n",
      "You are provided with a table and a question related to the table.\n",
      "Please generate a SQL query to answer this question. You should think step-by-step to reach the final SQL query.\n",
      "\n",
      "## Table\n",
      "Day: (see Irregularities ) | Sunday Sōl (Sun)               | Monday Luna (Moon)           | Tues\n",
      "  output_seq: <Analyze>\n",
      "We are given a table with multiple columns and rows. The columns represent the days of the week in different languages and traditions.\n",
      " The rows represent different languages or dialects.\n",
      "\n",
      " The question is: \"what's the thursday iuppiter (jupiter) with friday venus (venus) being vendredi\"\n",
      "\n",
      "\n",
      "  workspace_id: \n",
      "  external_knowledge: \n",
      "  data: reasoning-table\n",
      "  data_source: reasoning-table\n",
      "  prompt: [{'content': \"## Objective\\nYou are provided with a table and a question related to the table.\\nPlease generate a SQL query to answer this question. You should think step-by-step to reach the final SQL query.\\n\\n## Table\\nDay: (see Irregularities ) | Sunday Sōl (Sun)               | Monday Luna (Moo\n",
      "  env_class: deepanalyze\n",
      "  reward_spec: {'function': array(['extract_tableqa_answer', 'llm_as_judgement_analyze'], dtype=object), 'ground_truth': \"```sql SELECT `Thursday Iuppiter (Jupiter)` FROM days WHERE `Friday Venus (Venus)` = 'vendredi';```\", 'method': 'qa', 'response': '<Analyze>\\nWe are given a table with multiple columns and rows\n",
      "\n",
      "================================================================================\n",
      "FILE: RL/reseach.parquet\n",
      "================================================================================\n",
      "Shape: (9246, 9)  |  Columns: ['input_seq', 'output_seq', 'workspace_id', 'external_knowledge', 'data', 'data_source', 'prompt', 'env_class', 'reward_spec']\n",
      "Dtypes:\n",
      "input_seq                str\n",
      "output_seq               str\n",
      "workspace_id             str\n",
      "external_knowledge       str\n",
      "data                     str\n",
      "data_source              str\n",
      "prompt                object\n",
      "env_class                str\n",
      "reward_spec           object\n",
      "\n",
      "--- First row ---\n",
      "  input_seq: ## Instruction\n",
      " Generate a data insight report that clearly separates the 'Data Description' section, which details the contents and structure of the provided CSV files (authors, jobs, publishers, employee, pub_info, stores, discounts, titles, roysched, sales, titleauthor), from the 'Data Analysis' \n",
      "  output_seq: \n",
      "  workspace_id: book_publishing_company\n",
      "  external_knowledge: \n",
      "  data: research\n",
      "  data_source: research\n",
      "  prompt: [{'content': '## Instruction\\n Generate a data insight report that clearly separates the \\'Data Description\\' section, which details the contents and structure of the provided CSV files (authors, jobs, publishers, employee, pub_info, stores, discounts, titles, roysched, sales, titleauthor), from the\n",
      "  env_class: deepanalyze\n",
      "  reward_spec: {'function': array(['llm_as_judgement_opendomain'], dtype=object), 'ground_truth': \"authors:\\n      au_id au_lname au_fname        phone              address       city state   zip contract\\n172-32-1176    White  Johnson 408 496-7223      10932 Bigge Rd. Menlo Park    CA 94025        0\\n213-46-8915 \n",
      "\n",
      "================================================================================\n",
      "FILE: RL/reseach_small.parquet\n",
      "================================================================================\n",
      "Shape: (2000, 9)  |  Columns: ['input_seq', 'output_seq', 'workspace_id', 'external_knowledge', 'data', 'data_source', 'prompt', 'env_class', 'reward_spec']\n",
      "Dtypes:\n",
      "input_seq                str\n",
      "output_seq               str\n",
      "workspace_id             str\n",
      "external_knowledge       str\n",
      "data                     str\n",
      "data_source              str\n",
      "prompt                object\n",
      "env_class                str\n",
      "reward_spec           object\n",
      "\n",
      "--- First row ---\n",
      "  input_seq: ## Instruction\n",
      "Generate a data insight report using the data from cars.csv and trains.csv. Outline should clearly separate 'Data Description' (summarizing key columns, data types, and sample observations from both datasets) and 'Data Analysis' (investigating patterns, relationships, and trends, such\n",
      "  output_seq: \n",
      "  workspace_id: trains\n",
      "  external_knowledge: \n",
      "  data: research\n",
      "  data_source: research\n",
      "  prompt: [{'content': '## Instruction\\nGenerate a data insight report using the data from cars.csv and trains.csv. Outline should clearly separate \\'Data Description\\' (summarizing key columns, data types, and sample observations from both datasets) and \\'Data Analysis\\' (investigating patterns, relationship\n",
      "  env_class: deepanalyze\n",
      "  reward_spec: {'function': array(['llm_as_judgement_opendomain'], dtype=object), 'ground_truth': 'cars:\\n id  train_id  position     shape   len      sides   roof  wheels load_shape  load_num\\n  1         1         1 rectangle short not_double   none       2     circle         1\\n  2         1         2 rectangle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rl_files = sorted(glob.glob(os.path.join(DATA_ROOT, 'RL', '*.parquet')))\n",
    "\n",
    "for fpath in rl_files:\n",
    "    fname = os.path.basename(fpath)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"FILE: RL/{fname}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    df = pd.read_parquet(fpath)\n",
    "    print(f\"Shape: {df.shape}  |  Columns: {list(df.columns)}\")\n",
    "    print(f\"Dtypes:\\n{df.dtypes.to_string()}\")\n",
    "    print(f\"\\n--- First row ---\")\n",
    "    for col in df.columns:\n",
    "        val = str(df[col].iloc[0])[:300]\n",
    "        print(f\"  {col}: {val}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepanalyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
